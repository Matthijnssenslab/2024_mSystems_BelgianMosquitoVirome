


import pandas as pd
import seaborn as sns; sns.set(color_codes=True)
import numpy as np
import scipy
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import fcluster
import scipy.cluster.hierarchy as shc
import matplotlib.patches as mpatches
from sklearn.cluster import AgglomerativeClustering
from collections import namedtuple
from numpy import nan
from scipy.spatial.distance import pdist, squareform
from sklearn.metrics import silhouette_score


new_metadata = pd.read_csv(
    "data/BEmosq_metadata.csv",
    header=0,
    sep=";"
)
new_metadata


species_id = new_metadata[[
    "Sample",
    "visual_genus",
    "visual_species",
    "Municipality"
]]

species_id.set_index("Sample", inplace=True)
species_id.dropna(how='all', inplace= True)
species_id


from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable
from matplotlib import colorbar

def clean_data(dd_tsv, metadata):
    '''
    remove trailing suffix and replace column name spaces with underscores
    '''
    
    dd_tsv.columns = dd_tsv.columns.str.replace(" ", "_")

    # Add prefix columns and size
    dd_tsv["Sample_1"] = dd_tsv.apply(
        lambda row: row["Sample_1"],
        axis=1
    )
    dd_tsv["Sample_2"] = dd_tsv.apply(
        lambda row: row["Sample_2"],
        axis=1
    )

    # re-organize columns
    dd_tsv = dd_tsv[
        [
            "Sample_1",
            "Sample_2",
            "Matches",
            "Mismatches",
            "Jaccard_Index",
            "Mash-like_distance",
            "SNPs",
            "SNP_distance"
        ]
    ]
    
    # Add row to keep last sample
    uniques_sample_2 = [
    y
    for y in dd_tsv["Sample_2"].unique()
    ]
    
    last_sample = uniques_sample_2[-1]
    
    last_row = {
        "Sample_1":last_sample, 
        "Sample_2":last_sample, 
        "Matches":0, 
        "Mismatches":0, 
        "Jaccard_Index":0, 
        "Mash-like_distance":0, 
        "SNPs":0, 
        "SNP_distance":0
    }
    
    dd_tsv=dd_tsv.append(last_row, ignore_index=True)
    return dd_tsv


def create_pivot(df, values="SNP_distance", diagonal=0, remove_water=False):
    ''' create pivot tables with sample 1 and sample 2 correlations'''

    dd_tsv_pivot = df.pivot(
        columns="Sample_1",
        index="Sample_2",
        values=values
    )
    dd_tsv_pivot = dd_tsv_pivot.reindex(dd_tsv_pivot.columns)
    np.fill_diagonal(dd_tsv_pivot.values, diagonal)
    dd_tsv_pivot = dd_tsv_pivot.fillna(0) + dd_tsv_pivot.T.fillna(0) - np.diag(dd_tsv_pivot.values.diagonal())

    # remove waters from pivot table
    if remove_water:
        t_raw_piv_no_waters = dd_tsv_pivot[~(dd_tsv_pivot.index.str.contains("ater"))]
        t_raw_piv_no_waters_col = t_raw_piv_no_waters.drop(
            columns=t_raw_piv_no_waters.columns[
                t_raw_piv_no_waters.columns.str.contains("ater")
            ]
        )
    
        return t_raw_piv_no_waters_col
    
    else:
        return dd_tsv_pivot


def add_metadata_to_pivot(df, metadata_fields, metadata):
    """join metadata onto pivot table for axis labeling"""

    dd_tsv_pivot_w_labels = pd.merge(
        df.reset_index(),
        metadata[metadata_fields],
        how='left',
        left_on="Sample_1",
        right_index=True,
    ).reset_index().set_index(
        ["Sample_1"] + metadata_fields
    ).drop(["index"], axis=1)

    return dd_tsv_pivot_w_labels


def get_linkage(pivot_df):
    '''clustering linkage'''

    cdist = scipy.spatial.distance.squareform(pivot_df)
    return scipy.cluster.hierarchy.linkage(cdist, method="ward")


def get_cluster_map(pivot_df, linkage, num_clusters, extra=True):
    """cluster distance matrix and re-assign species as cluster species mode"""

    clusters = fcluster(linkage, num_clusters, criterion='maxclust')
    cluster_map = pd.DataFrame()
    
    if extra:
        cluster_map['Sample'] = pivot_df.index.get_level_values(0)
        #cluster_map['visual_genus'] = pivot_df['visual_genus'].tolist()
        #cluster_map['visual_species'] = pivot_df['visual_species'].tolist()
        cluster_map['visual_genus'] = pivot_df.index.get_level_values(1)
        cluster_map['visual_species'] = pivot_df.index.get_level_values(2)
        cluster_map['Municipality'] = pivot_df.index.get_level_values(3)

    cluster_map["Sample_1"] = pivot_df.columns#.drop(['visual_species', 'visual_genus'])
    cluster_map['cluster'] = clusters
    cluster_map.set_index("Sample_1", inplace=True)

    cluster_chunks = []
    for clust in cluster_map.cluster.unique():
        subset = cluster_map[cluster_map.cluster == clust]
        species_mode = subset.visual_species.mode()[0]
        genus_mode = subset[
            subset["visual_species"] == species_mode
        ].visual_genus.values[0]
        subset["ska_species"] = species_mode
        subset["ska_genus"] = genus_mode
        cluster_chunks.append(subset)

    cluster_map_w_ska = pd.concat(cluster_chunks)
    return cluster_map_w_ska


def join_on_ska_labels(raw_df, metadata, k, values="SNP_distance", diagonal=0):
    '''create pivot from distance matrix with old and new species labels'''

    df_cleaned = clean_data(
        raw_df,
        metadata=metadata
    )

    dd_pivot = create_pivot(
        df_cleaned,
        values=values,
        diagonal=diagonal
    )

    metadata_fields = [
        "visual_genus",
        "visual_species",
        "Municipality"
    ]

    dd_pivot_w_labels = add_metadata_to_pivot(
        dd_pivot,
        metadata_fields,
        metadata=metadata
    )

    linkage = get_linkage(dd_pivot)

    cluster_map = get_cluster_map(
        dd_pivot_w_labels,
        linkage,
        k,
        extra=True
    )

    ska_metadata_fields = metadata_fields + ["ska_genus", "ska_species"]

    final_pivot_w_labels = add_metadata_to_pivot(
        dd_pivot,
        ska_metadata_fields,
        metadata=cluster_map
    )

    return final_pivot_w_labels, cluster_map, linkage


def wrap_clustermap_and_mismatches(
    raw_df,
    metadata,
    k=4,
    figsize=(35, 35),
    values="SNP_distance",
    diagonal=0,
    fontsize=35,
    labelsize=25
):
    '''create clustermap with new and old species assigned labels'''

    pivot_w_labels, cluster_map, linkage = join_on_ska_labels(
        raw_df, 
        metadata, 
        k, 
        values=values, 
        diagonal=0
    )
    
    c, leg, cbar = correlation_matrix(
        pivot_w_labels,
        linkage,
        cluster_map,
        figsize=figsize,
        fontsize=fontsize,
        labelsize=labelsize
    )

    return c, leg, cbar, pivot_w_labels

def hierarchical_clustering(
    raw_df,
    metadata,
    k,
    values="SNP_distance",
    diagonal=0
):
    pivot_df, cluster_map, linkage = join_on_ska_labels(
        raw_df, 
        metadata, 
        k, 
        values=values,
        diagonal=diagonal
    )
    cdist = scipy.spatial.distance.squareform(pivot_df)
    Z = shc.linkage(cdist, method="ward")
    fig = plt.figure(figsize=(25, 10))

    dn = shc.dendrogram(
        Z,
        leaf_font_size=8,
        labels=pivot_df.index.get_level_values(4),
        color_threshold=0.008
    )

    return dn


def get_silhouette_score(
    raw_df,
    metadata,
    k,
    values="SNP_distance",
    diagonal=0,
    correlation=False
):
    '''use silhouette metric to see how compact and distinct clusters are '''
    
    pivot_df, cluster_map, linkage = join_on_ska_labels(
        raw_df, 
        metadata, 
        tailing, 
        k, 
        values=values,
        diagonal=diagonal
    )
    cdist = scipy.spatial.distance.squareform(pivot_df)
    linkage = scipy.cluster.hierarchy.linkage(cdist, method="ward")
    clusters = fcluster(linkage, k, criterion='maxclust')

    cluster_map = pd.DataFrame()

    cluster_map["Sample_1"] = pivot_df.columns.drop(['visual_species', 'visual_genus'])
    cluster_map['cluster'] = clusters
    cluster_map.set_index("Sample_1", inplace=True)

    return silhouette_score(
        pivot_df,
        metric="precomputed",
        labels=cluster_map["cluster"]
    )


def correlation_matrix(
    pivot_w_labels,
    linkage,
    cluster_map,
    figsize=(35, 35),
    fontsize=35,
    labelsize=25
):
    '''correlation matrix comparing visual and ska species assignment'''

    legend_labeling = {
         "Culex pipiens pipiens": "#54C568FF",#"#4c72a5",
         "Culex pipiens molestus": "#1FA188FF",#"#48a365",
         "Culex pipiens undetermined": "#FDE725FF",#"#77bedb",
         "Culex torrentium": "#BBDF27FF",#"#90ce9b",
         "Aedes japonicus": "#2A788EFF",#"#ccc197",
    }
    
    location_labeling = {
        "Bertem": "#0D0887FF",      
        "Dilsen-Stokkem": "#47039FFF",              
        "Eupen": "#7301A8FF",
        "Frameries": "#9C179EFF",
        "Kallo": "#BD3786FF",
        "Leuven": "#D8576BFF",
        "Maasmechelen": "#ED7953FF",
        "Natoye": "#FA9E3BFF",
        "Villers-Le-Bouillet": "#FDC926FF",
        "Vrasene": "#F0F921FF"                                      
    }

    pivot_index = pivot_w_labels.reset_index()
    pivot_index['visual_species'] = pivot_index['visual_species']
    pivot_index['ska_species'] = pivot_index['ska_species']
    pivot_index['Municipality'] = pivot_index['Municipality']

    key_colors_original = pivot_index["visual_species"].map({
        k.split(" ", 1)[1]: v
        for k, v in legend_labeling.items()
    })
    #key_colors_ska = pivot_index["ska_species"].map({
    #    k.split(" ", 1)[1]: v
    #    for k, v in legend_labeling.items()
    #})
    #key_colors_location = pivot_index["Municipality"].map({
    #    k: v
    #    for k, v in location_labeling.items()
    #})
    
    key_colors_labels = pd.DataFrame.from_dict(
        {
            "index": pivot_index["Sample_1"].values,
            #"Location": key_colors_location,
            "Species": key_colors_original#,      
            #"PCR": key_colors_original,
            #"SKA": key_colors_ska
        }
    ).set_index("index")
    
    c = sns.clustermap(
            pivot_w_labels.reset_index().drop([
                "visual_genus",
                "visual_species",
                "ska_genus",
                "ska_species",
                "Municipality"
            ], axis=1).set_index("Sample_1"),
            metric="correlation",
            cmap="mako",
            row_colors=key_colors_labels,
            col_colors=key_colors_labels,
            figsize=figsize,
            col_linkage=linkage,
            row_linkage=linkage,
            #xticklabels=pivot_w_labels.index.get_level_values(0),
            #yticklabels=pivo_w_labels.index.get_level_values(0)
            xticklabels=False,
            yticklabels=False
    )
    
    # make colorbar labels larger
    c.ax_col_colors.tick_params(labelright=False, right=False)
    c.ax_row_colors.tick_params(labelbottom=False)
    c.ax_col_colors
    
    # add seperating white line between colorbars
    c.ax_col_colors.axhline(1, linewidth=2, color="white")
    c.ax_row_colors.axvline(1, linewidth=2, color='white')
    
    # disable existing scale (too small)
    c.cax.set_visible(False)
    
    # add colorbar legend
    legend_patches = [
        mpatches.Patch(
            color=c, label=sp
        )
        for sp, c in sorted(legend_labeling.items())
    ]
    
    #location_patches = [
    #    mpatches.Patch(
    #        color=c, label=sp
    #    )
    #    for sp, c in sorted(location_labeling.items())
    #]
    
    ax = c.ax_heatmap
    ax.set_xlabel("")
    ax.set_ylabel("")
    leg = c.ax_heatmap.legend(
        loc='upper left',
        bbox_to_anchor=(1.10, 1.0),
        handles=legend_patches,
        frameon=False,
        fontsize=fontsize,
        prop={'size': fontsize, 'style': 'italic'}
    )
    
    #loc = c.ax_heatmap.legend(
    #    loc='lower left',
    #    bbox_to_anchor=(1.10, 1.0),
    #    handles=location_patches,
    #    frameon=False,
    #    fontsize=35,
    #    prop={'size': 35}
    #)

    # add in larger SNP Distance scale
    cax = c.fig.add_axes([0.05, 0.7, 0.02, 0.15])
    cbar = c.fig.colorbar(
        c.ax_heatmap.get_children()[0], 
        cax=cax, 
        orientation="vertical",
    )
    cbar.set_label(label="SNP Distance", fontsize=fontsize)
    cbar.ax.tick_params(labelsize=labelsize, width=0, length=0)
    cax.yaxis.set_label_position('left') 
    
    return c, leg, cbar



t_raw = pd.read_csv("data/ska_ksize_15.distances.tsv", sep="\t")

# Remove sample MEMO011 because it doesn't have enoough mosquito reads ==> skews clustering
t_raw=t_raw[~t_raw.stack().str.contains('MEMO011').any(level=0)]
t_raw


uniques_sample_1 = [
    y
    for y in t_raw["Sample 1"].unique()
]

uniques_sample_2 = [
    y
    for y in t_raw["Sample 2"].unique()
]
#uniques_sample_1
uniques_sample_2[-1]


# count in both samples 1 and 2 for some non-repeated data in original raw ska df
#[x for x in species_id.index if x not in uniques_sample_1 + uniques_sample_2]

t_raw_cleaned = clean_data(t_raw, species_id)
# Add last row to tsv file comparing last sample to itself, SNP distance 0!
t_raw_piv = create_pivot(t_raw_cleaned)


# check for missing samples:
#print([x for x in species_id.index if x not in t_raw_piv.index])


t_raw_piv


metadata_fields = [
    "visual_genus",
    "visual_species",
    "Municipality"
]

dd_pivot_w_labels = add_metadata_to_pivot(
    t_raw_piv,
    metadata_fields,
    metadata=species_id
)
dd_pivot_w_labels


linkage = get_linkage(t_raw_piv)


cluster_map = get_cluster_map(
    dd_pivot_w_labels,
    linkage,
    4,
    extra=True
)
cluster_map


ska_metadata_fields = metadata_fields + ["ska_genus", "ska_species"]

final_pivot_w_labels = add_metadata_to_pivot(
    t_raw_piv,
    ska_metadata_fields,
    metadata=cluster_map
)
final_pivot_w_labels


c, leg, cbar, final_pivot_w_labels = wrap_clustermap_and_mismatches(
    t_raw,
    species_id,
    k=4,
    figsize=(35,35),
    values="SNP_distance",
    diagonal=0,
    fontsize=35,
    labelsize=25
)

final_pivot_w_labels.query(
    'visual_species != ska_species'
)


c.savefig(
    "figures/species_assignment_clustermap.pdf",
    format="pdf",
    bbox_inches='tight',
    dpi=300
)
